{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state_dim = 20\n",
    "        self.action_dim = 3\n",
    "        \n",
    "        self.replay_buffer_size = 600\n",
    "        self.exp_idx = 0\n",
    "        self.state_buffer = np.zeros((self.replay_buffer_size, self.state_dim))\n",
    "        self.action_buffer = np.zeros((self.replay_buffer_size))\n",
    "        self.new_state_buffer = np.zeros((self.replay_buffer_size, self.state_dim))\n",
    "        self.reward_buffer = np.zeros((self.replay_buffer_size))\n",
    "        self.done_buffer = np.zeros((self.replay_buffer_size))\n",
    "        self.batch_exp_size = 32\n",
    "        \n",
    "        self.reward_discount = 0.95\n",
    "        self.learning_rate = 0.0001\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_min = 0.01\n",
    "        self.exploration_decay = 0.995\n",
    "        \n",
    "        self.update_nn = self.build_nn()\n",
    "        self.target_nn = self.build_nn()\n",
    "    \n",
    "    \n",
    "    def build_nn(self):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.LSTM(units=256, input_shape=(self.state_dim, 1), return_sequences=True))\n",
    "        model.add(keras.layers.LSTM(units=128, return_sequences=False))\n",
    "        model.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(units=3, activation=\"linear\"))\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        if np.random.random() <= self.exploration_rate:\n",
    "            action = np.random.choice([1, 0, -1])\n",
    "        else:\n",
    "            action = self.update_nn.predict(state)\n",
    "            action = action[0]\n",
    "            action = np.argmax(action)-1\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def store_experience(self, state, action, new_state, daily_reward, done):\n",
    "        idx = self.exp_idx % self.replay_buffer_size\n",
    "        \n",
    "        self.state_buffer[idx] = state\n",
    "        self.action_buffer[idx] = action\n",
    "        self.new_state_buffer[idx] = new_state\n",
    "        self.reward_buffer[idx] = daily_reward\n",
    "        self.done_buffer[idx] = done\n",
    "        \n",
    "        self.exp_idx += 1\n",
    "        \n",
    "    \n",
    "    def train_update_nn(self):\n",
    "        if self.exp_idx <= self.batch_exp_size:\n",
    "            return\n",
    "        \n",
    "        batch_exp = random.sample(range(0, min(self.replay_buffer_size, self.exp_idx)), self.batch_exp_size)\n",
    "        \n",
    "        states = []\n",
    "        q_values = []\n",
    "        \n",
    "        for idx in batch_exp:\n",
    "            state = self.state_buffer[idx]\n",
    "            action = self.action_buffer[idx]\n",
    "            new_state = self.new_state_buffer[idx]\n",
    "            reward = self.reward_buffer[idx]\n",
    "            done = self.done_buffer[idx]\n",
    "            \n",
    "            if done == 1:\n",
    "                target = reward\n",
    "            else:\n",
    "                new_state = np.reshape(new_state, (1, len(new_state), 1))\n",
    "                target = reward + self.reward_discount*np.amax(self.target_nn.predict(new_state)[0])\n",
    "            \n",
    "            state = np.reshape(state, (1, len(state), 1))\n",
    "            q_value = self.update_nn.predict(state)\n",
    "            q_value[0][int(action+1)] = target\n",
    "            \n",
    "            state = np.reshape(state, (20, 1))\n",
    "            q_value = np.reshape(q_value, (3))\n",
    "            \n",
    "            states.append(state)\n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        q_values = np.array(q_values)\n",
    "        \n",
    "        self.update_nn.fit(x=states, y=q_values, epochs=1, batch_size=32, verbose=1)\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate*self.exploration_decay)\n",
    "        \n",
    "        \n",
    "    def train_target_nn(self):\n",
    "        self.target_nn.set_weights(self.update_nn.get_weights()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
